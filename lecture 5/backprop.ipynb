{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the dataset randomly into 80% train set, 10% dev set, 10% test set.\n",
    "# Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "import random # shuffle the list of words to get an even distribution\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "random.seed(230)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the words to indexes\n",
    "chars = sorted(list(set(''.join(words)))) # get the unique characters through the set() method\n",
    "stoi = {s:i +1 for i,s in enumerate(chars)} # string to index\n",
    "stoi['.'] = 0 # end character\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()} # index to string\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD THE DATASET\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  xs, ys = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      xs.append(context)\n",
    "      ys.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  xs = torch.tensor(xs)\n",
    "  ys = torch.tensor(ys)\n",
    "  return xs, ys\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOILERPLATE CODE DONE\n",
    "\n",
    "# Utility function that compares our manually calculated gradients to pytorch (assumed source of truth) to check accuracy\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET UP MODEL PARAMETERS\n",
    "gain = 5/3 # we need a gain because we are using the tanh activation function; squashes, add a gain to get back to normal std\n",
    "dim_emb = 10 # dimensionality of the embedding\n",
    "n_hidden = 64 # number of hidden units\n",
    "\n",
    "# set up the model drivers\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "emb_lookup = torch.randn((len(chars) + 1, dim_emb), generator=g) #also written as 'C'. Can scale up dimensionality to capture more nuanced patterns\n",
    "\n",
    "# layer one\n",
    "W1 = torch.randn((dim_emb * block_size, n_hidden), generator=g)  * (gain / (dim_emb * block_size) **0.5) \n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
    "\n",
    "# layer two\n",
    "W2 = torch.randn((n_hidden, len(chars) + 1), generator=g) * (gain / n_hidden**0.5) # ending back up with 27 possible outputs / chars\n",
    "b2 = torch.randn(len(chars) + 1, generator=g) * 0\n",
    "\n",
    "# batchnorm parameters\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0 # scale\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1 # shift\n",
    "\n",
    "# put all of the parameters in one array for neatness -- you can sum all these to get total param count\n",
    "parameters = [emb_lookup, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7302, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = emb_lookup[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb)), but implementing manually again\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean() # this indexing expression selects one element from each row of logprobs using a pair of arrays (range(n) and Yb) for indexing. For each example i, it picks out logprobs[i, Yb[i]], which is the log probability of the true class Yb[i] for the i-th example. This creates a 1D array where each element corresponds to the log probability assigned by the model to the true class of each example.\n",
    "\n",
    "# PyTorch backward pass so we can run comparisons against it\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 10])\n",
      "tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
      "         -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
      "        [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
      "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01],\n",
      "        [ 9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0731e-02,\n",
      "          2.4968e+00,  2.4448e+00, -6.7006e-01, -1.2199e+00,  3.0314e-01],\n",
      "        [-1.0725e+00,  7.2762e-01,  5.1114e-02,  1.3095e+00, -8.0220e-01,\n",
      "         -8.5042e-01, -1.8068e+00,  1.2523e+00, -1.2256e+00,  1.2165e+00],\n",
      "        [-9.6478e-01, -2.3211e-01, -3.4762e-01,  3.3244e-01, -1.3263e+00,\n",
      "          1.1224e+00,  5.9641e-01,  4.5846e-01,  5.4011e-02, -1.7400e+00],\n",
      "        [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
      "         -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
      "        [ 1.3506e-01, -2.1161e-01, -1.0406e+00, -1.5367e+00,  9.3743e-01,\n",
      "         -8.8303e-01,  1.7457e+00,  2.1346e+00, -8.5614e-01,  5.4082e-01],\n",
      "        [ 6.1690e-01,  1.5160e+00, -1.0447e+00, -6.6414e-01, -7.2390e-01,\n",
      "          1.7507e+00,  1.7530e-01,  9.9280e-01, -6.2787e-01,  7.7023e-02],\n",
      "        [-1.1641e+00,  1.2473e+00, -2.7061e-01, -1.3635e+00,  1.3066e+00,\n",
      "          3.2307e-01,  1.0358e+00, -8.6249e-01, -1.2575e+00,  9.4180e-01],\n",
      "        [-1.3257e+00,  1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,\n",
      "          1.1491e+00, -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01],\n",
      "        [-1.0188e+00, -8.3777e-01, -2.1057e+00, -2.6044e-01, -1.7149e+00,\n",
      "         -3.3787e-01, -1.8263e+00, -8.3897e-01, -1.5723e+00,  4.5795e-01],\n",
      "        [-5.6533e-01,  5.4281e-01,  1.7549e-01, -2.2901e+00, -7.0928e-01,\n",
      "         -2.9284e-01, -2.1803e+00,  7.9311e-02,  9.0187e-01,  1.2028e+00],\n",
      "        [-5.6144e-01, -1.3753e-01, -1.3799e-01, -2.0977e+00, -7.9238e-01,\n",
      "          6.0689e-01, -1.4777e+00, -5.1029e-01,  5.6421e-01,  9.6838e-01],\n",
      "        [-3.1114e-01, -3.0603e-01, -1.7495e+00, -1.6335e+00,  3.8761e-01,\n",
      "          4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00],\n",
      "        [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
      "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
      "        [ 5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
      "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01],\n",
      "        [-2.1259e+00,  9.6041e-01,  1.2482e+00,  2.5341e-01,  2.8188e+00,\n",
      "         -3.3983e-01,  7.0311e-01,  4.0716e-01, -1.9018e-01, -6.9652e-01],\n",
      "        [ 1.7039e+00,  7.4204e-01,  9.7370e-01,  3.0028e-01, -2.8971e-01,\n",
      "         -3.1566e-01, -8.7898e-01,  1.0661e-01,  1.8598e+00,  5.5752e-02],\n",
      "        [ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
      "          1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
      "        [-2.1286e-01,  5.0950e-01,  3.2713e-01,  1.9661e+00, -2.4091e-01,\n",
      "         -7.9515e-01,  2.7198e-01, -1.1100e+00, -4.5285e-01, -4.9578e-01],\n",
      "        [ 1.2648e+00,  1.4625e+00,  1.1199e+00,  9.9539e-01, -1.2353e+00,\n",
      "          7.3818e-01,  8.1415e-01, -7.3806e-01,  5.6714e-01, -1.4601e+00],\n",
      "        [-2.4780e-01,  8.8282e-01, -8.1004e-02, -9.5299e-01, -4.8838e-01,\n",
      "         -7.3712e-01,  7.0609e-01, -1.9295e-01,  1.2348e+00,  3.3308e-01],\n",
      "        [ 1.3283e+00, -1.0921e+00, -8.3952e-01,  1.9098e-01, -7.1750e-01,\n",
      "         -3.8668e-01, -1.2542e+00,  1.2068e+00, -1.7102e+00, -4.7701e-01],\n",
      "        [-1.0527e+00, -1.4367e-01, -2.7737e-01,  1.1634e+00, -6.6910e-01,\n",
      "          6.4918e-01,  5.8243e-01,  1.9264e+00, -3.7846e-01,  7.9577e-03],\n",
      "        [ 5.1068e-01,  7.5927e-01, -1.6086e+00, -1.6065e-01,  1.3784e+00,\n",
      "         -2.7804e-01,  2.0710e-01,  1.0033e+00, -5.9772e-01, -3.9771e-01],\n",
      "        [-1.2801e+00,  9.2445e-02,  1.0526e-01, -3.9072e-01,  3.1723e-02,\n",
      "         -5.4753e-01,  8.1827e-01, -8.1628e-01, -3.9243e-01, -7.4521e-01],\n",
      "        [-9.4649e-01, -1.5941e-01, -1.9336e-01, -3.7660e-01, -4.9158e-02,\n",
      "          9.3866e-02, -6.4533e-01,  1.2108e+00, -7.8198e-01,  3.8449e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(emb_lookup.shape)\n",
    "print(emb_lookup)\n",
    "\n",
    "# basically, 27 characters, with ten different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n",
      "tensor([[12, 15, 26],\n",
      "        [ 1, 19, 25],\n",
      "        [ 0,  0,  3],\n",
      "        [ 0,  0,  0],\n",
      "        [ 1, 12,  4],\n",
      "        [ 0,  0,  0],\n",
      "        [12,  5, 18],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [14,  1, 13],\n",
      "        [ 1,  4,  1],\n",
      "        [ 0, 11,  5],\n",
      "        [ 2,  5, 25],\n",
      "        [ 0, 11,  1],\n",
      "        [12, 25, 12],\n",
      "        [ 0, 26, 19],\n",
      "        [11,  1, 18],\n",
      "        [ 5,  1, 12],\n",
      "        [ 1,  4, 25],\n",
      "        [ 1, 20,  8],\n",
      "        [ 0,  0,  0],\n",
      "        [12, 13,  1],\n",
      "        [ 0,  0,  3],\n",
      "        [19, 21, 18],\n",
      "        [ 8,  5, 15],\n",
      "        [15, 18, 18],\n",
      "        [ 2,  9,  7],\n",
      "        [ 0,  0,  5],\n",
      "        [18,  1,  6],\n",
      "        [ 0,  0, 23],\n",
      "        [22,  9, 15],\n",
      "        [13,  1, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(Xb.shape)\n",
    "print(Xb)\n",
    "# 32 examples, 3 characters in each minibatch, where each integer represents the index of the character in the stoi dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777, -0.5103,\n",
       "         0.5642,  0.9684], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_lookup[12]\n",
    "\n",
    "# this is the final lookup table that contains the row vectors for each character in the minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5614, -0.1375, -0.1380, -2.0977, -0.7924,  0.6069, -1.4777,\n",
       "          -0.5103,  0.5642,  0.9684],\n",
       "         [ 0.5557,  0.4746, -1.3867,  1.6229,  0.1720,  0.9885,  0.5066,\n",
       "           1.0198, -1.9062, -0.4275],\n",
       "         [-0.9465, -0.1594, -0.1934, -0.3766, -0.0492,  0.0939, -0.6453,\n",
       "           1.2108, -0.7820,  0.3845]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_lookup[Xb[:1]]\n",
    "\n",
    "# go through row by row, concenate the emb_lookup per indicated indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  8,  7, 15,  1,  0, 16, 10,  0, 12,  5, 12, 23,  1, 15, 13,  0,\n",
       "        14,  1,  9,  8,  1,  9, 18, 25, 25, 14,  1,  5, 14,  9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb # when getting the loss, you're going to Yb, plucking out each row (only 1 in this case), then taking the log probabilities for each item, and then taking the mean of that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly, this is loss:\\\n",
    "loss = -logprobs[range(n), Yx].mean()\\\n",
    "= -(probA + probB + probC) / 3\\\n",
    "= (-1/3)probA + (-1/3)probB + (-1/3)probC\\\n",
    "Thus, dloss / dproba = -1/3, assuming A is the true class because the other items are false classes and don't contribute to the loss. We assign a gradient to A so that the model will update the probability of A next time.\n",
    "\n",
    "This generalizes to dloss / dlogprobs = (-1/n) for all n True class items.\\\n",
    "Let's say that logprobs = ((-1)probA + (-1)probB + (-1)probC ... + (-1)probN) \\\n",
    "You're looking for the correct label and the gradient changes based on that, so loss is actually the negative log probability of the correct label. So you end up with:\\\n",
    "-1 / n, because all the other ones get derived out.\n",
    "\n",
    "Note that when we say dlogprobs, this is dloss / dlogprobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m cmp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, dprobs, probs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# probs = counts * counts_sum_inv, e.g. with shapes [32, 27] * [32, 1] \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# this means that there are two operations: 1) replicating columns in counts_sum_inv, and 2) multiplying\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# we first backpropagate through the multiplication then through the replication.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Recall in micrograd if multiple inputs arrive at a node, you sum. \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# the columns of counts_sum_inv are used multiple times (32) in each column in counts so we'll sum horizontally each column in counts\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m dcounts_sum_inv \u001b[38;5;241m=\u001b[39m (\u001b[43mcounts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdprobs\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# [32, 27], then summing all columns (item 1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# notice also that some gradient is flowing from csi -> probs, so we need to chain through those as well\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# dL / dcounts = dprobs / dcounts * dL / dprobs. dprobs / dcounts = counts_sum_inv(counts**0)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m dcounts \u001b[38;5;241m=\u001b[39m dprobs \u001b[38;5;241m*\u001b[39m counts_sum_inv\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs) # empty gradients for the log probabilities\n",
    "dlogprobs[range(n), Yb] = -1/n # add gradient to all true classes\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "# chain rule -> dprobs wrt loss = dprobs wrt logprobs * dlogprobs wrt loss\n",
    "dprobs = (1.0 / probs) # dlogprobs / dprobs\n",
    "dprobs = dprobs * dlogprobs\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "# probs = counts * counts_sum_inv, e.g. with shapes [32, 27] * [32, 1] \n",
    "# this means that there are two operations: 1) replicating columns in counts_sum_inv, and 2) multiplying\n",
    "# we first backpropagate through the multiplication then through the replication.\n",
    "# Recall in micrograd if multiple inputs arrive at a node, you sum. \n",
    "# the columns of counts_sum_inv are used multiple times (32) in each column in counts so we'll sum horizontally each column in counts\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True) # [32, 27], then summing all columns (item 1)\n",
    "# notice also that some gradient is flowing from csi -> probs, so we need to chain through those as well\n",
    "# dL / dcounts = dprobs / dcounts * dL / dprobs. dprobs / dcounts = counts_sum_inv(counts**0)\n",
    "dcounts = dprobs * counts_sum_inv\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "# counts_sum_inv = counts_sum^-1\n",
    "dcounts_sum = -(counts_sum**-2) * dcounts_sum_inv\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# backproprogate through the sum operation\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum # multiplying each row (torch.ones just to get the right shape), each element in a given row gets same gradient\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "# counts = norm_logits.exp() eg. e^x for every element in norm_logits\n",
    "# for every element in norm_logits, f(x) = e^x. f'(x) = e^x, still.\n",
    "# so with the chain rule, dL / dnorm_logits = dL / dcounts * dcounts / dnorm_logits\n",
    "dnorm_logits = dcounts * norm_logits.exp()\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "# norm_logits = logits - logit_maxes\n",
    "# dL / dlogit_maxes = dnorm_logits / dlogit_maxes * dL / dnorm_logits\n",
    "# logit_maxes is being replicated again, also\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdims=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "# dL / dlogits = dnorm_logits / dlogits * dL / dnorm_logits\n",
    "dlogits = dnorm_logits.clone()\n",
    "\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# this gets the highest of the values inside a given row inside the logits tensor and also squashes down into a [32, 1] tensor\n",
    "# dL / dlogits = dlogits / dlogit_maxes * dL / dlogit_maxes\n",
    "# we broadcast additional gradients to the maxes; to do this we encode the location with F.one_hot\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "# logits = h @ W2 + b2. This is a matrix multiplication (dot product) of h with W2, then adding b2\n",
    "# matrix multiplication has higher precedence, and then addition\n",
    "# thus we backpropagate through the addition first, then the matrix multiplication\n",
    "# db2 is just a passthrough, but it has shape [27], not [32, 27], so we sum the gradients\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('b2', db2, b2)\n",
    " \n",
    "# gradient of h will have shape [32, 64] compared to logits [32, 27]\n",
    "# each element of h is affected by an entire row of W2 in the forward pass\n",
    "# so in the backward pass, we need to reverse this operation by multiplying dlogits by W2.t()\n",
    "dh = dlogits @ W2.t()\n",
    "cmp('h', dh, h)\n",
    "\n",
    "# h.t() is first because matrix multiplication requires the inner dimensions to match\n",
    "dW2 = h.t() @ dlogits\n",
    "cmp('W2', dW2, W2)\n",
    "\n",
    "# h = torch.tanh(hpreact) # hidden layer\n",
    "# dL / dhpreact = dL / dh * dh / dhpreact\n",
    "# reversing the tanh operation -> y = tanh(z). dy / dz = 1 - y^2\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "# bngain is [1, 64]; bnraw is [32, 64]; bnbias is [1, 64]; hpreact is [32, 64]\n",
    "# so bnraw is broadcasted to hpreact shape, replicating rows in bngain.\n",
    "# we first backpropagate through the multiplication then through the replication.\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdims=True)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "\n",
    "dbnraw = bngain * dhpreact\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "dbnvar_inv = (bndiff*dbnraw).sum(0, keepdims=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# = (bnvar + 10^-5)^-0.5\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "# there is a sum operation here, so we need to backpropagate through that\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum # multiplying each row (torch.ones just to get the right shape), each element in a given row gets same gradient\n",
    "dbndiff2 = torch.ones_like(bndiff2) * ((n-1)**-1) * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "# bndiff2 = bndiff**2\n",
    "dbndiff += 2 * bndiff * dbndiff2 \n",
    "cmp('bndiff', dbndiff, bndiff) # didn't call this until later because dbndiff not finished proprogating yet\n",
    "\n",
    "# bndiff = hprebn - bnmeani\n",
    "# we need to sum the gradients for the rows \n",
    "dbnmeani = -dbndiff.sum(0, keepdims=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "dhprebn = dbndiff.clone()\n",
    "\n",
    "# bnmeani = (1/n)*hprebn.sum(0, keepdim=True)\n",
    "# this is a sum and then a multiple, so backprop multiply first then spread into all elements\n",
    "# sum already happened with the dbndiff clone -- correct dimensions\n",
    "dhprebn += (1/n) * dbnmeani\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "# hprebn = embcat @ W1 + b1\n",
    "dembcat = dhprebn @ W1.t()\n",
    "cmp('embcat', dembcat, embcat)\n",
    "\n",
    "dW1 = embcat.t() @ dhprebn\n",
    "cmp('W1', dW1, W1) \n",
    "\n",
    "db1 = dhprebn.sum(0)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "# this basically reshapes the emb tensor into a 2D tensor, combining the y and z dimensions into one\n",
    "# instead, reverse it and proprogate the dembcat straight through, but in the updated shape\n",
    "demb = dembcat.view(emb.shape)\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "# emb = emb_lookup[Xb]\n",
    "# Xb is the indices of the characters in the minibatch for which embeddings are needed\n",
    "# emb_lookup is the embedding matrix\n",
    "# indices in Xb are used to select the corresponding rows from emb_lookup. For each index in Xb, the corresponding row (embedding vector) from emb_lookup is selected.\n",
    "# Forward pass operations: 1) for each index in Xb, select corresponding row from emb_lookup. 2) concetenate the rows to form a 2D tensor.\n",
    "# Each row in emb_lookup is used multiple times -> sum of the gradients of all outputs it influences. This summing reflects the total contribution of that vector to the loss across all its occurrences in the batch\n",
    "# So, I should count how many of each index in Xb, multiply corresponding gradients\n",
    "# I don't need to actually think about what index is in Xb - the gradients are already computed and flowing back - I just need to attach the gradients calaculated to each element in Xb, and add\n",
    "\n",
    "demb_lookup = torch.zeros_like(emb_lookup) # to store gradients for each vocab char in emb_lookup\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]): # iterating through every character of Xb\n",
    "        demb_lookup[Xb[i, j]] += demb[i, j]\n",
    "\n",
    "cmp('emb_lookup', demb_lookup, emb_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: better understand when to sum and when to average gradients.\n",
    "# I start getting confused when I have a couple vectors of very different shapes to transport gradients between"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
