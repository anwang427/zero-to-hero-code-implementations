{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing; what happens if all Weights & Biases are initialized to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the dataset randomly into 80% train set, 10% dev set, 10% test set.\n",
    "# Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "import random # shuffle the list of words to get an even distribution\n",
    "import torch\n",
    "\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "random.seed(230)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the words to indexes\n",
    "chars = sorted(list(set(''.join(words)))) # get the unique characters through the set() method\n",
    "stoi = {s:i +1 for i,s in enumerate(chars)} # string to index\n",
    "stoi['.'] = 0 # end character\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()} # index to string\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  xs, ys = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      xs.append(context)\n",
    "      ys.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  xs = torch.tensor(xs)\n",
    "  ys = torch.tensor(ys)\n",
    "  return xs, ys\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET UP MODEL PARAMETERS\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "gain = 5/3 # we need a gain because we are using the tanh activation function; squashes, add a gain to get back to normal std\n",
    "dim_emb = 2 # dimensionality of the embedding\n",
    "n_hidden = 100 # number of hidden units\n",
    "\n",
    "# set up the model drivers\n",
    "g = torch.Generator().manual_seed(2147483648)\n",
    "emb_lookup = torch.randn((len(chars) + 1, dim_emb), generator=g) #also written as 'C'. Can scale up dimensionality to capture more nuanced patterns\n",
    "W1 = torch.zeros(dim_emb * block_size, n_hidden) * (gain / (dim_emb * block_size) **0.5) \n",
    "W2 = torch.zeros(n_hidden, len(chars) + 1) * (gain / n_hidden**0.5)\n",
    "b2 = torch.zeros(len(chars) + 1) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden)) # scale\n",
    "bnbias = torch.zeros((1, n_hidden)) # shift\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # running mean\n",
    "bnstd_running = torch.ones((1, n_hidden)) # running std\n",
    "\n",
    "# put all of the parameters in one array for neatness -- you can sum all these to get total param count\n",
    "parameters = [emb_lookup, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "# map parameters to names\n",
    "named_parameters = [\n",
    "    ('emb_lookup', emb_lookup), \n",
    "    ('W1', W1), \n",
    "    ('W2', W2), \n",
    "    ('b2', b2), \n",
    "    ('bngain', bngain), \n",
    "    ('bnbias', bnbias)\n",
    "]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8466,  1.1186],\n",
       "        [-0.1104,  0.8080],\n",
       "        [ 1.1208,  0.6045],\n",
       "        [-0.2629, -0.8401],\n",
       "        [-1.2115,  1.5081],\n",
       "        [-0.3306, -0.3676],\n",
       "        [ 0.4935, -1.0824],\n",
       "        [-1.1730, -0.0739],\n",
       "        [-0.3131, -0.1387],\n",
       "        [ 0.0370, -0.1308],\n",
       "        [ 0.3529,  0.4702],\n",
       "        [-0.3813, -1.2174],\n",
       "        [-0.9630,  0.1225],\n",
       "        [-0.0106,  1.3502],\n",
       "        [ 0.0903,  0.9139],\n",
       "        [ 1.0598, -1.1161],\n",
       "        [-0.2989, -0.1344],\n",
       "        [-1.3220, -0.4518],\n",
       "        [-0.5018,  0.5162],\n",
       "        [-0.7952,  1.0163],\n",
       "        [-0.5378,  1.3582],\n",
       "        [ 0.2305, -2.1728],\n",
       "        [ 1.4393,  0.3626],\n",
       "        [-1.6578,  0.1090],\n",
       "        [ 0.4156, -0.9011],\n",
       "        [-0.7543,  0.3030],\n",
       "        [-2.1870, -0.6238]], requires_grad=True)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 20\n",
    "loss_values = []\n",
    "learning_rates = []\n",
    "steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7952,  1.0163],\n",
      "         [-0.7543,  0.3030],\n",
      "         [ 0.0903,  0.9139]],\n",
      "\n",
      "        [[-0.7952,  1.0163],\n",
      "         [-0.3131, -0.1387],\n",
      "         [-0.1104,  0.8080]],\n",
      "\n",
      "        [[ 0.0370, -0.1308],\n",
      "         [-0.7543,  0.3030],\n",
      "         [-0.1104,  0.8080]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186]],\n",
      "\n",
      "        [[ 0.0370, -0.1308],\n",
      "         [ 0.0903,  0.9139],\n",
      "         [-0.1104,  0.8080]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-0.7952,  1.0163]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-0.1104,  0.8080],\n",
      "         [-0.3131, -0.1387]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-0.5378,  1.3582]],\n",
      "\n",
      "        [[ 0.0370, -0.1308],\n",
      "         [-0.7952,  1.0163],\n",
      "         [-0.3131, -0.1387]],\n",
      "\n",
      "        [[ 0.0370, -0.1308],\n",
      "         [-0.7952,  1.0163],\n",
      "         [-0.0106,  1.3502]],\n",
      "\n",
      "        [[-0.9630,  0.1225],\n",
      "         [ 0.0370, -0.1308],\n",
      "         [-0.1104,  0.8080]],\n",
      "\n",
      "        [[-0.1104,  0.8080],\n",
      "         [-0.0106,  1.3502],\n",
      "         [-0.3306, -0.3676]],\n",
      "\n",
      "        [[-0.1104,  0.8080],\n",
      "         [-0.9630,  0.1225],\n",
      "         [-0.7543,  0.3030]],\n",
      "\n",
      "        [[-0.0106,  1.3502],\n",
      "         [-0.1104,  0.8080],\n",
      "         [ 0.0903,  0.9139]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-0.2989, -0.1344]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [ 1.0598, -1.1161],\n",
      "         [-0.5018,  0.5162]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [ 0.0903,  0.9139]],\n",
      "\n",
      "        [[-0.5378,  1.3582],\n",
      "         [-0.5378,  1.3582],\n",
      "         [ 1.0598, -1.1161]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-1.2115,  1.5081]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-0.3813, -1.2174],\n",
      "         [-0.3306, -0.3676]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186]],\n",
      "\n",
      "        [[-0.5018,  0.5162],\n",
      "         [-0.3306, -0.3676],\n",
      "         [ 1.1208,  0.6045]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-0.5018,  0.5162],\n",
      "         [-0.1104,  0.8080]],\n",
      "\n",
      "        [[-0.0106,  1.3502],\n",
      "         [-0.3306, -0.3676],\n",
      "         [ 0.0903,  0.9139]],\n",
      "\n",
      "        [[-0.3813, -1.2174],\n",
      "         [-0.3306, -0.3676],\n",
      "         [-0.3306, -0.3676]],\n",
      "\n",
      "        [[ 1.0598, -1.1161],\n",
      "         [ 0.0903,  0.9139],\n",
      "         [-0.5378,  1.3582]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186]],\n",
      "\n",
      "        [[-0.0106,  1.3502],\n",
      "         [-0.1104,  0.8080],\n",
      "         [ 0.4156, -0.9011]],\n",
      "\n",
      "        [[-0.9630,  0.1225],\n",
      "         [-0.1104,  0.8080],\n",
      "         [ 0.0903,  0.9139]],\n",
      "\n",
      "        [[-0.9630,  0.1225],\n",
      "         [ 0.0370, -0.1308],\n",
      "         [-0.1104,  0.8080]],\n",
      "\n",
      "        [[-1.8466,  1.1186],\n",
      "         [-1.8466,  1.1186],\n",
      "         [-0.5378,  1.3582]],\n",
      "\n",
      "        [[ 1.0598, -1.1161],\n",
      "         [-1.2115,  1.5081],\n",
      "         [-0.1104,  0.8080]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]], grad_fn=<StdBackward0>)\n",
      "tensor([[[-0.2629, -0.8401],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[ 0.3529,  0.4702],\n",
      "         [ 0.2305, -2.1728],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [-2.1870, -0.6238]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[ 0.3529,  0.4702],\n",
      "         [    nan,     nan],\n",
      "         [-2.1870, -0.6238]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [ 0.4935, -1.0824],\n",
      "         [ 0.4935, -1.0824]],\n",
      "\n",
      "        [[ 0.4935, -1.0824],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [ 1.4393,  0.3626],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [-1.6578,  0.1090],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[-1.1730, -0.0739],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [-1.3220, -0.4518],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan],\n",
      "         [    nan,     nan],\n",
      "         [    nan,     nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan],\n",
      "         [nan, nan]]], grad_fn=<IndexBackward0>)\n",
      "Preactivations: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)\n",
      "Batch Mean: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<MeanBackward1>)\n",
      "Batch STD: tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## TRAINING THE MODEL\n",
    "\n",
    "# Basically the same thing as running a validation / test run except with gradient updates\n",
    "for k in range(max_steps): #running a loop as we're running a bunch of forward passes to get loss and optimize\n",
    "    ## Make a mini batch\n",
    "    # In this example, we will grab 32 rows of the lookup table\n",
    "    # This is random, which can help with generalization\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "    \n",
    "    ## Forward pass\n",
    "    # embed the input xs tensor into the lookup table\n",
    "    # instead of grabbing the entire dataset as one batch to train, we get mini batches\n",
    "    xs_embeddings = emb_lookup[Xtr[ix]] # feed into the two subsequent layers\n",
    "    ### Batch Normalization Layer\n",
    "\n",
    "    # standardizing these so that they are roughly gaussian before tanh\n",
    "    # guassian distribution: inherantly lower probability of extreme values\n",
    "    # should only be forced to be gaussian at intialization, not during training -- we want distribution to be able to move around\n",
    "    # so we introduce 'scale' and 'shift' parameters to the network\n",
    "    pre_activations = xs_embeddings.view(-1, 6) @  W1\n",
    "    print(f'Preactivations: {pre_activations}')\n",
    "    # define a forward hook that captures the outputs of intermediate layers\n",
    "\n",
    "    # estimate batch statistics as we go so that we can pass one input through easily during val later\n",
    "    bnmeani = pre_activations.mean(0, keepdim=True)\n",
    "    bnstdi = pre_activations.std(0, keepdim=True)\n",
    "    print(f'Batch Mean: {bnmeani}')\n",
    "    print(f'Batch STD: {bnstdi}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani # nudging a bit toward current batch\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi # no_grad as we aren't edting activations wrt these\n",
    "\n",
    "    pre_activations = bngain * (pre_activations - bnmeani) / bnstdi + bnbias\n",
    "    \n",
    "\n",
    "    # continue feeding into the subsequent layers\n",
    "    hidden_layer = torch.tanh(pre_activations)\n",
    "    logits = hidden_layer @ W2 + b2 #matrix multiplication, give us the log counts\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # print(loss.item())\n",
    "\n",
    "    # TODO: need to understand the L2 regularization from part one. Also less clear about functions below\n",
    "    # For example: 1) why are we going through all the parameters, and 2) why aren't we updating weights.data, but all\n",
    "\n",
    "    ## Backward pass   \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Inspect the gradients\n",
    "    '''\n",
    "    for name, p in named_parameters:\n",
    "        if p.grad is not None:\n",
    "            print(f'{name} grad: {p.grad}')\n",
    "        else:\n",
    "            print(f'{name} has no grad yet')\n",
    "    '''\n",
    "            \n",
    "    ## Update\n",
    "    for p in parameters:\n",
    "        p.data += -(0.003) * p.grad\n",
    "    steps.append(k)\n",
    "    loss_values.append(loss.log10().item())   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing what happens when weights and biases are set to zero\n",
    "It's so interesting that some of the weights are [0, 0] and other are [nan, nan].\n",
    "What exactly are these?\n",
    "\n",
    "parameters = [emb_lookup, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "It mostly seems like nothing is training at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CHECK HIDDEN LAYER ACTIVATIONS\n",
    "# There are many very extreme values, which will all get squashed\n",
    "\n",
    "plt.hist(pre_activations.view(-1).tolist(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK ACTIVATION FUNCTION VALUES\n",
    "# This is something that goes wrong in initializing nn's - too many +-1's\n",
    "# This means that inputs to the tanh are two extreme, so they are getting squashed\n",
    "# As the activation function approaches +-1, the gradient (ie. derivation of the function) approaches 0\n",
    "# A backward pass function: self.grad += (1-t**2) * out.grad\n",
    "\n",
    "plt.hist(hidden_layer.view(-1).tolist(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe7e1ae6e20>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT HIDDEN LAYER ACTIVATIONS\n",
    "# If there is an entire column of white, that means we have a dead neuron because the backward gradient was destroyed\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(hidden_layer.abs() > 0.99, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT STEPS TO LOSS RELATIONSHIP (Iteration Two, After Increasing Model Size)\n",
    "\n",
    "plt.plot(steps, loss_values)\n",
    "plt.title(\"Loss By Step\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT EMBEDDINGS (Iteration Three)\n",
    "# TODO: I don't know why this is returning embedded characters, yet.\n",
    "# When characters are close to each other the neural net is treating them similarly\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(emb_lookup[:, 0].data, emb_lookup[:,1].data, s=200) # each word has two dimensions / columns\n",
    "for i in range(emb_lookup.shape[0]):\n",
    "    plt.text(emb_lookup[i, 0].item(), emb_lookup[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train nan\n",
      "val nan\n"
     ]
    }
   ],
   "source": [
    "# CHECK TOTAL LOSS ON TRAIN_SET FORWARD PASS\n",
    "\n",
    "# Do a forward pass (without updates)\n",
    "@torch.no_grad()\n",
    "\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "\n",
    "    xs_embeddings = emb_lookup[x]\n",
    "    pre_activations = xs_embeddings.view(xs_embeddings.shape[0], -1) @  W1 \n",
    "    pre_activations = bngain * (pre_activations - bnmean_running) / bnstd_running + bnbias\n",
    "    hidden_layer = torch.tanh(pre_activations)\n",
    "\n",
    "    logits = hidden_layer @ W2 + b2 # matrix multiplication, give us the log counts\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mria.\n",
      "mayahlieel.\n",
      "ndynyal.\n",
      "rethrstend.\n",
      "leg.\n",
      "aderedieliileli.\n",
      "jelle.\n",
      "eisennanar.\n",
      "kayziohlara.\n",
      "noshubergahi.\n",
      "jest.\n",
      "jair.\n",
      "jelilentenof.\n",
      "uba.\n",
      "ghde.\n",
      "jyleli.\n",
      "ehs.\n",
      "kay.\n",
      "myskeyah.\n",
      "hil.\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE FROM MODEL\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        embeddings = emb_lookup[torch.tensor([context])]\n",
    "        hidden_layer = torch.tanh(embeddings.view(1, -1) @ W1 + b1)\n",
    "        logits = hidden_layer @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample from distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "\n",
    "        # Shift context window and track samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "\n",
    "        # Break if end token detected\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
