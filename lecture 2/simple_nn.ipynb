{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn',\n",
       " 'abigail']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(word) for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(word) for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <END>\n",
      "<START> o\n",
      "o l\n",
      "l i\n",
      "i v\n",
      "v i\n",
      "i a\n",
      "a <END>\n",
      "<START> a\n",
      "a v\n",
      "v a\n",
      "a <END>\n"
     ]
    }
   ],
   "source": [
    "# developing a weak bigram language model (only looking for two characters at a time, looking at one char and predicting next one)\n",
    "\n",
    "for word in words[:3]: # demonstrate on first word\n",
    "    # in this word, iterate through it to get two chars at a time\n",
    "    with_special_characters = ['<START>'] + list(word) + ['<END>'] # separate each word with special characters\n",
    "    for ch1, ch2 in zip(with_special_characters, with_special_characters[1:]): # interate through word, and word starting at second character so that you don't get (e, e) for emma for example\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how often each bigram occurs -- stats\n",
    "# need dict to maintain count\n",
    "\n",
    "bigram_counts = {} # empty dict\n",
    "for word in words:\n",
    "    # in this word, iterate through it to get two chars at a time\n",
    "    with_special_characters = ['<START>'] + list(word) + ['<END>'] # separate each word with special characters\n",
    "    for ch1, ch2 in zip(with_special_characters, with_special_characters[1:]): # interate through word, and word starting at second character so that you don't get (e, e) for emma for example\n",
    "       bigram_counts_index = (ch1, ch2) # tuple\n",
    "       bigram_counts[bigram_counts_index] = bigram_counts.get(bigram_counts_index, 0) + 1 # get the count of the bigram, if it doesn't exist, return 0 and add 1 to it\n",
    "\n",
    "# This will return something like:\n",
    "# {('<START>', 'e'): 1}\n",
    "# {('<START>', 'e'): 1, ('e', 'm'): 1}\n",
    "# {('<START>', 'e'): 1, ('e', 'm'): 1, ('m', 'm'): 1}\n",
    "# {('<START>', 'e'): 1, ('e', 'm'): 1, ('m', 'm'): 1, ('m', 'a'): 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<END>'), 6763),\n",
       " (('a', '<END>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<START>', 'a'), 4410),\n",
       " (('e', '<END>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<START>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<START>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<END>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<START>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<END>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<START>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<END>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<START>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<START>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<START>', 'l'), 1572),\n",
       " (('<START>', 'c'), 1542),\n",
       " (('<START>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<END>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<END>'), 1314),\n",
       " (('<START>', 't'), 1308),\n",
       " (('<START>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<END>'), 1169),\n",
       " (('<START>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<START>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<START>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<END>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<START>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<START>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<START>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<END>'), 516),\n",
       " (('d', '<END>'), 516),\n",
       " (('<START>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<END>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<START>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<START>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<START>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<END>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<START>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<END>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<END>'), 160),\n",
       " (('u', '<END>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<START>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<END>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<END>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<END>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<START>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<END>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<END>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<START>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<END>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<END>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<END>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<END>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigram_counts.items(), key = lambda kv: -kv[1]) # kv: -kv[1] sorts by the second item in the dict, ie. the count. Negative sorts descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp38-none-macosx_10_9_x86_64.whl (146.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 146.7 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from torch) (4.8.0)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.3-cp38-cp38-macosx_10_9_x86_64.whl (13 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, fsspec, filelock, torch\n",
      "Successfully installed MarkupSafe-2.1.3 filelock-3.12.4 fsspec-2023.9.2 jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((3, 5), dtype=torch.int32) # example\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0] = 1 # this is how you set a value in a tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (3.7.3)\n",
      "Collecting numpy==1.24.1\n",
      "  Downloading numpy-1.24.1-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.8 MB 5.4 MB/s eta 0:00:01     |██████████████████              | 11.2 MB 2.4 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (0.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed numpy-1.24.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929,    0],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435,    0],\n",
       "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
       "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
       "            0,   83,    0,    0],\n",
       "        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n",
       "          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n",
       "            3,  104,    4,    0],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n",
       "            0,  317,    1,    0],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181,    0],\n",
       "        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n",
       "            0,   14,    2,    0],\n",
       "        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n",
       "           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n",
       "            0,   31,    1,    0],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "            0,  213,   20,    0],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277,    0],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n",
       "            0,   10,    0,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n",
       "            0,  379,    2,    0],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "            0, 1588,   10,    0],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n",
       "            0,  287,   11,    0],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145,    0],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54,    0],\n",
       "        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n",
       "            0,   12,    0,    0],\n",
       "        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n",
       "            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n",
       "            0,    0,    0,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23,    0],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "            0,  215,   10,    0],\n",
       "        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n",
       "          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105,    0],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45,    0],\n",
       "        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n",
       "           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n",
       "            0,  121,    0,    0],\n",
       "        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n",
       "            0,   73,    1,    0],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n",
       "           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n",
       "           38,   30,   19,    0],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78,    0],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net = torch.zeros((28, 28), dtype=torch.int32) # building neural net of character array\n",
    "\n",
    "unique_chars = list(set(''.join(words))) # get all unique characters and transform into list\n",
    "alphabetrical_unique_chars = sorted(unique_chars)\n",
    "stoi = {ch: i+1 for i, ch in enumerate(alphabetrical_unique_chars)} # string to index\n",
    "itos = {i: ch for ch, i in stoi.items()} # index to string\n",
    "stoi['<START>'] = 26\n",
    "stoi['<END>'] = 27\n",
    "stoi['.'] = 0\n",
    "itos[0] = '.'\n",
    "\n",
    "# transform into array\n",
    "for word in words:\n",
    "    # in this word, iterate through it to get two chars at a time\n",
    "    with_special_characters = ['.'] + list(word) + ['.'] # separate each word with special characters\n",
    "    for ch1, ch2 in zip(with_special_characters, with_special_characters[1:]): # interate through word, and word starting at second character so that you don't get (e, e) for emma for example\n",
    "       # accessed slightly differently from first example given pytorch constraints\n",
    "       index_one = stoi[ch1]\n",
    "       index_two = stoi[ch2]\n",
    "       neural_net[index_one, index_two] += 1 # filling out the empty neural net array with the character counts\n",
    "\n",
    "neural_net ## identical to the weights we have later, but filled in by counting, whereas in gradient optimization we initialize randomly then let loss guide us to the right numbers\n",
    "## also a valid way to train, probably just not very scalable / efficient. If you take more context (ex. 3 chars, or 4) this array just expands and expands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929,    0], dtype=torch.int32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0., 4410., 1306., 1542., 1690., 1531.,  417.,  669.,  874.,  591.,\n",
       "        2422., 2963., 1572., 2538., 1146.,  394.,  515.,   92., 1639., 2055.,\n",
       "        1308.,   78.,  376.,  307.,  134.,  535.,  929.,    0.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_vector = neural_net[0].float() # convert into float to normalize the counts\n",
    "probability_vector / probability_vector.sum() # normalize the counts\n",
    "probability_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(10328361) # set seed for reproducibility\n",
    "probability_vector = torch.rand(3, generator=generator)\n",
    "probability_vector = probability_vector / probability_vector.sum()\n",
    "index = torch.multinomial(probability_vector, num_samples=1, replacement=True, generator=generator).item() # sample from the probability vector\n",
    "itos[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iae.\n",
      "nl.\n",
      "madan.\n",
      "ma.\n",
      "k.\n",
      "a.\n",
      "cha.\n",
      "satammeralesynda.\n",
      "noh.\n",
      "giceleyyn.\n",
      "avyr.\n",
      "s.\n",
      "lyh.\n",
      "borasy.\n",
      "t.\n",
      "ti.\n",
      "sh.\n",
      "la.\n",
      "zyn.\n",
      "rimi.\n",
      "dannn.\n",
      "vyaniahl.\n",
      "toyrvia.\n",
      "cowonichrzarilalllavaran.\n",
      "aicamelazezos.\n",
      "n.\n",
      "arha.\n",
      "n.\n",
      "aua.\n",
      "tan.\n",
      "sheilelynanah.\n",
      "listoro.\n",
      "teryomare.\n",
      "thayla.\n",
      "shantti.\n",
      "lyaneorielelecilos.\n",
      "akadevon.\n",
      "jeysee.\n",
      "kon.\n",
      "olaweyllavali.\n",
      "iraveismun.\n",
      "atenngtoto.\n",
      "di.\n",
      "ann.\n",
      "kishialay.\n",
      "bey.\n",
      "jeluelentu.\n",
      "kyath.\n",
      "liguxila.\n",
      "e.\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(10328361)\n",
    "\n",
    "for i in range(50):\n",
    "    out = []\n",
    "    index = 0\n",
    "\n",
    "    while True:\n",
    "        probability_vector = neural_net[index].float() # convert into float to normalize the counts, gets probabilities of chars in bigram neural net\n",
    "        probability_vector = probability_vector / probability_vector.sum()\n",
    "\n",
    "        ## uniform distribution that makes everything equally likely, not 'trained' like above\n",
    "        # probability_vector = torch.ones(28) / 28\n",
    "\n",
    "        index = torch.multinomial(probability_vector, num_samples=1, replacement=True, generator=generator).item() # gets a sample from the multinomial distribution, like m, or a\n",
    "        out.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "        \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# instead of a statistical model, now we are going to pass this into a neural network\n",
    "xs, ys = [], []\n",
    "\n",
    "for word in words[:1]: # Emma\n",
    "    with_special_characters = ['.'] + list(word) + ['.'] # separate each word with special characters\n",
    "    for ch1, ch2 in zip(with_special_characters, with_special_characters[1:]): # interate through word, and word starting at second character so that you don't get (e, e) for emma for example\n",
    "       index_one = stoi[ch1]\n",
    "       index_two = stoi[ch2]\n",
    "       print(ch1, ch2) # Five examples for neural network\n",
    "       xs.append(index_one)\n",
    "       ys.append(index_two)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the above, when input to neural network xs[0] = 0, we want ys[0] = 5 to have a very high probability, and so forth\n",
    "\n",
    "# Now, let's feed these examples into a neural network, where neural nodes have weights. It doesn't really make sense to feed integers into the data to multiply by the weights, we want vectors of floats\n",
    "# We should fill in a tensor (vector) which corresponds to that integer but represented in vector form\n",
    "# appropriate bits are one, everything else is zero\n",
    "\n",
    "import torch.nn.functional as F\n",
    "x_encoded = F.one_hot(xs, num_classes=27).float()\n",
    "x_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0199],\n",
       "        [ 0.0718],\n",
       "        [-0.5823],\n",
       "        [-0.9975],\n",
       "        [ 0.1654],\n",
       "        [ 0.0177],\n",
       "        [ 0.8005],\n",
       "        [ 0.1831],\n",
       "        [ 1.6134],\n",
       "        [-0.1869],\n",
       "        [ 0.0305],\n",
       "        [-0.1526],\n",
       "        [ 1.2124],\n",
       "        [-0.0083],\n",
       "        [ 0.0510],\n",
       "        [-1.6765],\n",
       "        [ 0.6808],\n",
       "        [-0.1999],\n",
       "        [-0.0611],\n",
       "        [-2.6551],\n",
       "        [-0.8364],\n",
       "        [-0.8796],\n",
       "        [ 0.3318],\n",
       "        [-0.1856],\n",
       "        [-0.2666],\n",
       "        [-0.2334],\n",
       "        [-0.6123]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights = torch.randn((27, 1)) # random initialized weights, as an example neural network\n",
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0199],\n",
       "        [ 0.0177],\n",
       "        [-0.0083],\n",
       "        [-0.0083],\n",
       "        [ 0.0718]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_encoded @ Weights # dot product; we're seeing the five activations of the weight (neuron) on the five inputs (x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5851,  9.2431,  0.4207,  0.9388,  0.4330,  0.5203,  0.4035,  5.7702,\n",
       "          0.6945,  1.0163,  0.1915, 12.5951,  0.9829,  1.8943,  1.2268,  3.8488,\n",
       "          2.7527,  1.7713,  2.5241,  4.3175,  0.9236,  2.1941,  1.8778,  0.5048,\n",
       "          2.6747,  2.0480,  2.3330],\n",
       "        [ 1.3708,  6.8282,  1.4400,  0.8908,  1.2035,  0.8037,  1.5452,  6.2404,\n",
       "          1.9076,  0.0745,  3.4202,  2.1633,  0.8062,  1.1703,  0.4999,  0.2970,\n",
       "          0.6500,  1.7589,  0.1513,  0.2544,  2.0055,  1.4690,  0.7045,  0.3609,\n",
       "          0.2396,  0.5369,  1.1744],\n",
       "        [ 0.9580,  0.1552,  3.3532,  0.1754,  0.7790,  0.5762,  0.6652,  0.1994,\n",
       "          0.3455,  0.5957,  0.6131,  0.2873,  1.6419,  1.6819,  1.0969,  7.2752,\n",
       "          2.3565,  2.1887,  1.8467,  0.7262,  1.6893,  0.3844,  2.5566,  1.6272,\n",
       "          1.5724,  6.7502,  0.5078],\n",
       "        [ 0.9580,  0.1552,  3.3532,  0.1754,  0.7790,  0.5762,  0.6652,  0.1994,\n",
       "          0.3455,  0.5957,  0.6131,  0.2873,  1.6419,  1.6819,  1.0969,  7.2752,\n",
       "          2.3565,  2.1887,  1.8467,  0.7262,  1.6893,  0.3844,  2.5566,  1.6272,\n",
       "          1.5724,  6.7502,  0.5078],\n",
       "        [ 0.7101,  0.2961,  0.4193,  1.0430,  0.5153,  2.0782,  0.4211,  0.6500,\n",
       "          0.3332,  0.9452,  3.1865,  1.4386,  0.7565,  0.2679,  3.5443,  0.7684,\n",
       "          1.7625,  0.5693,  1.7517,  0.9492,  0.2031,  0.3684,  0.9265,  0.5836,\n",
       "         14.0236,  1.0935,  0.1830]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_encoded @ Weights).exp() # get rid of the zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0178, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0254, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0271, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0444, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0232, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### Do a forward pass, the above is just demo code ####\n",
    "Weights = torch.randn((27, 27), generator = generator, requires_grad=True) # you need requires_grad or the autograd system won't track these tensors\n",
    "x_encoded = F.one_hot(xs, num_classes=27).float()\n",
    "logits = x_encoded @ Weights\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1, keepdims = True) # Gives us our probability tensor we'll index into for the next token\n",
    "\n",
    "# For example where xs = tensor([0, 5, 13, 13, 1]) and ys = tensor([5, 13, 13, 1, 0]) we are trying to find the probability of the character in the prob_tensor at index 5 (corresponding with ys)\n",
    "print(probs[0, 5]) # probability of the character in the prob_tensor at index 5 (corresponding with ys[0])\n",
    "\n",
    "print(probs[1, 13]) # probability of the character in the prob_tensor at index 13 (corresponding with ys[1])\n",
    "\n",
    "print(probs[2, 13]) # probability of the character in the prob_tensor at index 13 (corresponding with ys[2])\n",
    "\n",
    "print(probs[3, 1])\n",
    "\n",
    "print(probs[4, 0]) # and so forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0178, 0.0254, 0.0271, 0.0444, 0.0232], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way of writing the print statements above\n",
    "x_axis = torch.arange(5)\n",
    "probabilities = probs[x_axis, ys]\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6368, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing the log likelihood of the data -- we'll go for minimizing the negative log likelihood\n",
    "loss = -probabilities.log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Do a backward pass ####\n",
    "Weights.grad = None # reset the weights in the network\n",
    "loss.backward() # compute the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0051,  0.0023,  0.0044,  0.0010,  0.0024, -0.1964,  0.0051,  0.0189,\n",
       "          0.0092,  0.0041,  0.0135,  0.0044,  0.0035,  0.0091,  0.0223,  0.0141,\n",
       "          0.0007,  0.0038,  0.0076,  0.0038,  0.0049,  0.0087,  0.0040,  0.0122,\n",
       "          0.0045,  0.0231,  0.0033],\n",
       "        [-0.1954,  0.0034,  0.0010,  0.0156,  0.0149,  0.0064,  0.0055,  0.0160,\n",
       "          0.0065,  0.0168,  0.0018,  0.0159,  0.0043,  0.0059,  0.0024,  0.0016,\n",
       "          0.0130,  0.0058,  0.0065,  0.0100,  0.0118,  0.0013,  0.0018,  0.0060,\n",
       "          0.0055,  0.0057,  0.0100],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0061,  0.0050,  0.0018,  0.0033,  0.0177,  0.0017,  0.0046,  0.0028,\n",
       "          0.0021,  0.0103,  0.0125,  0.0108,  0.0031, -0.1949,  0.0067,  0.0021,\n",
       "          0.0007,  0.0103,  0.0041,  0.0192,  0.0015,  0.0047,  0.0218,  0.0087,\n",
       "          0.0177,  0.0126,  0.0031],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0081, -0.1822,  0.0297,  0.0066,  0.0074,  0.0063,  0.0049,  0.0127,\n",
       "          0.0328,  0.0207,  0.0243,  0.0151,  0.0528, -0.1891,  0.0035,  0.0021,\n",
       "          0.0013,  0.0154,  0.0034,  0.0077,  0.0021,  0.0449,  0.0018,  0.0096,\n",
       "          0.0148,  0.0394,  0.0041],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights.grad # Each elements tells us the influence of a given weight on the launch function; if the gradient is positive, increasing the weight will increase the loss, and if negative increasing the weight will reduce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#### Update the weights ####\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m Weights\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39;49m\u001b[39m0.1\u001b[39;49m \u001b[39m*\u001b[39;49m Weights\u001b[39m.\u001b[39;49mgrad \u001b[39m# update the weights, reducing them to decrease the loss\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "#### Update the weights ####\n",
    "Weights.data += -0.1 * Weights.grad # update the weights, reducing them to decrease the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5363, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If I now recalculate the forward pass, the loss should be lower\n",
    "x_encoded = F.one_hot(xs, num_classes=27).float()\n",
    "logits = x_encoded @ Weights\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1, keepdims = True) # Gives us our probability tensor we'll index into for the next token\n",
    "x_axis = torch.arange(5)\n",
    "probabilities = probs[x_axis, ys]\n",
    "loss = -probabilities.log().mean()\n",
    "loss\n",
    "\n",
    "# As you can see above, loss decreased by 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Now, let's put this all together to train a neural network over the entire list of names ######\n",
    "\n",
    "### 1. Build the dataset\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "xs, ys = [], []\n",
    "\n",
    "for word in words:\n",
    "    characters = ['.'] + list(word) + ['.'] # separate each word with special characters\n",
    "    for characters[1], characters[2] in zip(characters, characters[:1]):\n",
    "        index1 = stoi[characters[1]]\n",
    "        index2 = stoi[characters[2]]\n",
    "        xs.append(index1)\n",
    "        ys.append(index2)\n",
    "xs = torch.tensor(xs) # Corresponds to rows in the weights tensor\n",
    "ys = torch.tensor(ys) # Correspond to the columns matched up with the rows in the weights tensor, what the 'actual' outputs should be\n",
    "\n",
    "number_of_examples = xs.nelement() # should be 32033\n",
    "\n",
    "### 2. Initialize the toy network with randomized weights\n",
    "g = torch.Generator().manual_seed(10328361)\n",
    "Weights = torch.rand((27, 27), generator=g, requires_grad=True) # What network expects output of input x_axis to be\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09226502478122711\n",
      "0.09145938605070114\n",
      "0.09066691994667053\n",
      "0.08988770097494125\n",
      "0.08912106603384018\n",
      "0.08836685121059418\n",
      "0.08762483298778534\n",
      "0.08689450472593307\n",
      "0.08617596328258514\n",
      "0.08546879887580872\n",
      "0.0847725197672844\n",
      "0.08408711850643158\n",
      "0.08341237157583237\n",
      "0.08274776488542557\n",
      "0.08209340274333954\n",
      "0.08144878596067429\n",
      "0.08081398159265518\n",
      "0.08018859475851059\n",
      "0.07957233488559723\n",
      "0.07896526902914047\n",
      "0.07836687564849854\n",
      "0.0777774378657341\n",
      "0.07719624042510986\n",
      "0.07662355154752731\n",
      "0.0760587528347969\n",
      "0.07550226151943207\n",
      "0.07495330274105072\n",
      "0.074412040412426\n",
      "0.0738782212138176\n",
      "0.07335181534290314\n"
     ]
    }
   ],
   "source": [
    "### 3. Carry out gradient descent optimization (includes forward, backward, update). In a separate cell so that\n",
    "### we don't re-initialize the network every time rendering our update weights function useless\n",
    "\n",
    "for k in range(30): # Number of times we run the learning process\n",
    "    ## 3.1 Forward pass\n",
    "    x_encoded = F.one_hot(xs, num_classes=27).float() # one-hot encode the x-axis inputs\n",
    "    logits = x_encoded @ Weights # predict the log-counts of the outputs characters based on the randomized weights. Softmax!\n",
    "    counts = logits.exp() # get rid of the zeroes\n",
    "    probability_tensor = counts / counts.sum(1, keepdims = True) # probabilities of the outputs averaged over the entire dataset\n",
    "    loss = -probability_tensor[torch.arange(number_of_examples), ys].log().mean() # negative log likelihood (loss) of the data, which we are trying to minmize\n",
    "    print(loss.item())\n",
    "\n",
    "    ## 3.2 Backward pass\n",
    "    Weights.grad = None # Zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    ## 3.3 Update the weights -- given the loss is positive, we should try to decrease the weights\n",
    "    Weights.data += -0.1 * Weights.grad # update the weights, reducing them to decrease the loss\n",
    "\n",
    "    ## From here, every time you rerun this cell, it will update again. The -0.1 is the learning rate -- we could probably increase this, or we increase the number of times we run the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srellrvjdehqzgdshamxcnrvatsgionkkkjazlbqjjhcm.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "###### Now, we can sample from the trained neural net model (with updated weights per the gradient optimization) to see how it's actually performing qualitatively, as before! ######\n",
    "for i in range(5):\n",
    "    output = []\n",
    "    index = 0\n",
    "\n",
    "    while True:\n",
    "        x_encoded = F.one_hot(torch.tensor([index]), num_classes=27).float()\n",
    "        logits = x_encoded @ Weights # using the updated weights.data per adjustments via weights.grad after calculating the loss from that input data\n",
    "        counts = logits.exp()\n",
    "        probability_tensor = counts / counts.sum(1, keepdims=True) # probabilty distribution\n",
    "\n",
    "        index = torch.multinomial(probability_tensor, num_samples=1, replacement=True, generator=g).item() # sampling an index based on the given probability distribution; ie. higher probability more likely to be chosen but not guaranteed\n",
    "        output.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(output))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
